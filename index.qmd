---
title: "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital"
format: 
  html:
    number-sections: true
    toc: true
    embed-resources: true
    crossrefs-hover: false
    css: style.css
    code-copy: true
    lang: es
    bibliography: bibliografia/bibliografia.bib
    csl: bibliografia/ieee.csl
    cap-location: bottom
date: last-modified

language: 
  title-block-author-single: "Autor"

author:
  - name: Mauricio Acosta
    orcid: 
    corresponding: true
    email: mauriba491@gmail.com
    affiliations:
      - name: GISTAQ (UTN-FRRe)
        url: https://www.instagram.com/gistaq.utn/

abstract: |
  Este sitio web contiene cuestiones relacionadas a la estimación de la profundidad de disco de Secchi mediante técnicas de sensado remoto. Se presenta información proveniente de investigaciones previas y se discute la confección de modelos de regresión  mediante código de programación así como su evaluación mediante pruebas estadísticas.
keywords:
  - GISTAQ
  - UTN
  - FRRe
  - Quarto
---

# Revisión bibliográfica

Esta revisión tiene como objetivo identificar los enfoques más comunes en la literatura, los tipos de datos utilizados y las variables espectrales que se consideran más relevantes para este tipo de estimaciones.

## Profundidad de disco de Secchi

La profundidad de visibilidad del disco de Secchi $(SDD)$ es una medida ampliamente utilizada para evaluar la transparencia del agua. Se define como la profundidad en metros a la cual un disco blanco circular desaparece al dejarlo descender en un cuerpo de agua.

Esta variable es un indicador indirecto de la calidad del agua, ya que se relaciona con la concentración de sedimentos en suspensión, fitoplancton y otros materiales particulados. Continua siendo utilizado debido a la simplicidad y rango de aplicación universal del método, además de tratarse de un parámetro fácilmente entendible por el público.

## Métodos tradicionales de estimación

En las últimas décadas, se han desarrollado múltiples ecuaciones empíricas y modelos basados en datos in situ y sensores remotos para estimar esta profundidad de manera eficiente y a gran escala. El objetivo de recopilar es identificar patrones comunes, variables predictoras relevantes y estrategias metodológicas empleadas.

| Nombre                 | Variable           | Formula                                 | Muestras | R²   |
|------------------------|--------------------|-----------------------------------------|----------|------|
| Dekker and Peters      | $\mathrm{ln(SDD)}$ | $\mathrm{ln(Red)}$                      | 15       | 0.86 |
| Dominguez Gomez et al. | $\mathrm{SDD}$     | $\mathrm{(Green)^x}$                    | 16       | 0.90 |
| Giardino et al.        | $\mathrm{SDD}$     | $\mathrm{Blue/Green}$                   | 4        | 0.85 |
| Kloiber et al.         | $\mathrm{ln(SDD)}$ | $\mathrm{Blue/Red + Blue}$              | 374      | 0.93 |
| Lathrop and Lillesand  | $\mathrm{ln(SDD)}$ | $\mathrm{Green}$                        | 9        | 0.98 |
| Mancino et al.         | $\mathrm{SDD}$     | $\mathrm{Red/Green + Blue/Green + Blue}$| 60       | 0.82 |


: Tabla 1: Algoritmos publicados para la predicción de $\mathrm{SDD}$ utilizando la plataforma Landstat @Rubin. {.striped .hover .responsive}

En la mayoría de los casos la relación entre $SDD$ y la intensidad de la luz es no lineal, por lo que se utiliza $ln(SDD)$ para realizar la regresión.

La correlación con la banda roja puede explicarse causalmente por la correlación positiva directa entre la reflectancia en el rojo y la carga bruta de partículas que induce la dispersión de partículas. De manera que mientras que la claridad del agua $(SDD)$ desciende, la intensidad en el rojo aumenta @Matthews.

Como referencia se presentan las propiedades de las distintas bandas de la plataforma espacial Sentinel-2:

|  |  | Sentinel-2A | | Sentinel-2B | |
|:--|--:|--:|--:|--:|--:|
| **Banda** | **Resolución espacial (m)** | **Longitud de onda (nm)** | **Ancho de banda (nm)** | **Longitud de onda (nm)** | **Ancho de banda (nm)** |
| $\mathrm{B01}$ (aerosol) | 60 | 442.7 | 20 | 442.3 | 20 |
| $\mathrm{B02}$ (blue) | 10 | 492.7 | 65 | 492.3 | 65 |
| $\mathrm{B03}$ (green) | 10 | 559.8 | 35 | 558.9 | 35 |
| $\mathrm{B04}$ (red) | 10 | 664.6 | 38 | 664.9 | 31 |
| $\mathrm{B05}$ (red edge) | 20 | 794.1 | 14 | 703.8 | 15 |
| $\mathrm{B06}$ | 20 | 748.5 | 14 | 739.1 | 13 |
| $\mathrm{B07}$ | 20 | 782.8 | 19 | 779.7 | 19 |
| $\mathrm{B08}$ (NIR) | 10 | 832.8 | 105 | 832.9 | 104 |
| $\mathrm{B8A}$ | 20 | 864.7 | 21 | 864.0 | 21 |
| $\mathrm{B09}$ | 60 | 945.1 | 19 | 943.2 | 20 |
| $\mathrm{B10}$ | 60 | 1373.5 | 29 | 1376.9 | 29 |
| $\mathrm{B11}$ (SWIR 1) | 20 | 1613.7 | 90 | 1616.4 | 94 |
| $\mathrm{B12}$ (SWIR 2) | 20 | 2292.4 | 174 | 2185.7 | 184 |

: Tabla 2: Propiedades de las bandas S2-MSI, para las plataformas S2A y S2B. {.striped .hover .responsive tbl-colwidths="[20,20,15,15,15,15]"}

## Regresión lineal

En estadística, la regresión lineal es un modelo matemático usado para aproximar la relación de dependencia entre una variable dependiente $Y$ con $m$ variables independientes $X_i$. Este modelo puede ser expresado como:

$$ Y = \beta_0 +  \beta_1 X_1 + \cdots + \beta_m X_m$$

donde:

- $Y$ es la variable dependiente o variable de respuesta.
- $X_1,X_2,...X_m$ son las variables explicativas, independientes o regresoras.
- $\beta_0,\beta_1,\beta_2,...\beta_m$ son los parámetros del modelo, miden la influencia que las variables explicativas tienen sobre el regrediendo.

### Selección de variables

La seleccion de variables y características es el foco de mucha investigación. El objetivo de la selección de variables es mejorar el rendimiento de predicción de los predictores y proporcionar una mejor comprensión del proceso subyacente que generó los datos. 

La identificación de las variables más relevantes para incluir o excluir en un modelo predictivo es un paso fundamental en cualquier investigación rigurosa, especialmente cuando se busca construir modelos con alto poder explicativo y capacidad de generalización. 

Comprender qué variables contribuyen en mayor medida a la calidad de las predicciones permite no solo mejorar el desempeño del modelo, sino también interpretar con mayor claridad los fenómenos que se están estudiando.

No existe una forma segura de definir la importancia de una variable puesto que la utilidad de la misma depende del modelo implementado y de las demás variables con las que interactua: una variable que es completamente inutil por si misma puede resultar en una mejora del rendimiento significativa cuando es considerada junto con otras variables @guyon2003introduction. 

A pesar de las dificultades existen estrategias útiles para determinar un subconjunto de variables útiles para la predicción. Tal es el caso de los métodos *forward selection* y *backwards elimination*. En *forward selection*, las variables son progresivamente incorporadas, evaluando el modelo al paso de cada una, mientras que en *backwards elimination* uno empieza con el conjunto entero de variables y progresivamente elimina las menos prometedoras.

Una forma de implementación de *forward selection* es por empezar por un modelo exento de variables independientes, pero cuyo término independiente sea la media de las variables de respuesta. A partir de este valor se calcula el residuo como la diferencia entre el valor verdadero $Y$ y el valor predecido $Y_{pred}$.

$$r = Y - Y_{pred}$$

Seguidamente, se computa la correlación entre cada variable independiente $X_i$ con el residuo y se incorpora al modelo la variable con la mayor correlación absoluta. El proceso se repite esta vez con las predicciones del nuevo modelo y se continua hasta alcanzar una desempeño deseado o un número determinado de variables. Este método fue utilizado satisfactoriamente en investigaciones previas @BONANSEA2019102265.

![Fig. 1: Matriz de correlación entre las distintas bandas.](img/heatmap.svg){width=80%}

Una herramienta útil para ayudar en la selección de variables es una matriz de correlación. Dos variables prefectamente correlacionadas resultan redundantes en el sentido de que añadir ambas no aporta información adicional. Sin embargo, una correlación muy alta entre variables (o anti-correlación) no significa ausencia de complementariedad.

### Generación de características lineales y no lineales

Incluir nuevas variables a partir de las originales, como productos, cocientes o transformaciones no lineales (por ejemplo, logaritmos, o raíces cuadradas) permite capturar relaciones más complejas entre las variables independientes y la variable objetivo. Por ejemplo, si se dispone de datos espectrales, pueden construirse razones entre bandas (band ratios) o índices espectrales que resalten ciertas propiedades físicas del fenómeno estudiado. Estas nuevas características permiten al modelo lineal aproximar mejor la relación entre los datos y la salida, especialmente cuando la relación real no es perfectamente lineal.

# Regresión Lineal

La construcción del modelo se lleva a cabo utilizando la versión de Python 3.12 y las librerías previamente especificadas, detallando la función de cada pieza de código, observaciones, avances y descubrimientos.

## Librerías

Para regresión lineal se usan principalmente las librerías de Python *numpy*, *pandas* y *Scikit-learn*.

- **Scikit-learn**: está en el corazón de las operaciones de ciencias de datos en Python. Ofrece módulos para procesamiento de datos, aprendizaje supervisado y no supervisado, selección y validación de modelos, y métricas de error.
- **Pandas**: especializada en la manipulación y el análisis de datos. Ofrece estructuras de datos y operaciones para manipular tablas numéricas y series temporales.
- **NumPy**: provee al usuario con arreglos multidimensionales, junto a un gran conjunto de funciones para operar en estos.

## Lectura de datos

Para la carga, lectura y manipulación de la información la librería *Pandas* permite convertir un archivo .csv en un *DataFrame*, estructura de datos que facilita la manipulación de estos. El fragmento de código siguiente agrupa los valores de $SDD$ con los valores de reflectancia a diferentes longitudes de onda para los cuales coinciden fecha, latitud y longitud y crea un nuevo *DataFrame*.

```{python}
import pandas as pd

# Rutas a los archivos CSV
archivo_reflectancias = "datos\\base_de_datos_gis.csv"  # contiene: fecha,punto,pixel,banda,reflect,longitud,latitud
archivo_parametros = "datos\\base_de_datos_lab.csv"     # contiene: fecha,longitud,latitud,param,valor

# Leer los archivos
df_reflect = pd.read_csv(archivo_reflectancias)
df_param = pd.read_csv(archivo_parametros)

# Filtrar los parámetros "secchi"
df_secchi = df_param[df_param["param"].str.lower() == "secchi"]

# Merge por fecha y coordenadas
merged = pd.merge(
    df_secchi,
    df_reflect,
    on=["fecha", "latitud", "longitud"],
    how="inner"
)

# Pivotear la tabla para poner bandas como columnas
tabla_final = merged.pivot_table(
    index=["param", "fecha", "longitud", "latitud", "valor"], 
    columns="banda",
    values="reflect"
).reset_index()

# Reordenar columnas: param | B01 | B02 | ... | B8A
bandas = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12', 'B8A']
columnas_finales = ['valor'] + bandas

# Crear Tabla final
df = tabla_final[columnas_finales]
```

El *DataFrame* obtenido se asemeja a la siguiente tabla:

| $\mathrm{SDD}$  | $\mathrm{B01}$  | $\mathrm{B02}$  | $\cdots$  | $\mathrm{B12}$  | $\mathrm{B8A}$  |
| :--------------:| :--------------:| :--------------:| :--------:| :--------------:| :--------------:|
| 10              | 0,1728          | 0,1754          | $\cdots$  | 0,1404          | 0,1869          |
| 15              | 0,1497          | 0,17022         | $\cdots$  | 0,1113          | 0,1567          |
| $\vdots$        | $\vdots$        | $\vdots$        | $\cdots$  | $\vdots$        | $\vdots$        |
| 125             | 0,1571          | 0,1563          | $\cdots$  | 0,1419          | 0,1436          |scr
| 135             | 0,1503          | 0,1591          | $\cdots$  | 0,1420          | 0,1454          |

: Tabla 3: DataFrame de los datos recolectados. {.striped .hover .responsive}

## Análisis de los datos

La relación no lineal entre la penetración de la luz y la profundidad de disco de Secchi fue ya descrita por diversos autores que hallan una mejor descripción de esta como una de tipo logaritmica @modisComparison. Se verifica este comportamiento con las bandas B04, B05 y B06:

```{python}
#| echo: false
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
fig, ax = plt.subplots(ncols=2,nrows = 3, figsize=(8,7))

SDD = df.iloc[24:31,0]
log_SDD = np.log(SDD)
B04 = df.iloc[24:31,4]
B05 = df.iloc[24:31,5]
B06 = df.iloc[24:31,6]

# B04 SDD
ax[0,0].scatter(B04, SDD, color="#17A77E", alpha=0.7)
ax[0,0].plot([min(B04),max(B04)],[max(SDD),min(SDD)], color='gray', linestyle='--')
ax[0,0].set(ylabel="SDD")

# B04 LOG(SDD)
lm = LinearRegression().fit(B04.values.reshape(-1,1), log_SDD)
ax[0,1].scatter(B04, log_SDD, color="#9D50A6", alpha=0.7)
ax[0,1].plot([min(B04),max(B04)],[lm.predict([[min(B04)]]),lm.predict([[max(B04)]])], color='gray', linestyle='--')
ax[0,1].set(ylabel="ln(SDD)")
ax[0,1].text(0.01,0.02,f'ln(SDD) = {round(lm.coef_[0],2)} • B04 + {round(lm.intercept_,2)}', transform=ax[0,1].transAxes)
ax[0,1].text(0.01,0.09,f'R² = {round(r2_score(log_SDD,lm.predict(B04.values.reshape(-1,1))),2)}', transform=ax[0,1].transAxes)

# B05
ax[1,0].scatter(B05, SDD, color="#17A77E", alpha=0.7)
ax[1,0].plot([min(B05),max(B05)],[max(SDD),min(SDD)], color='gray', linestyle='--')
ax[1,0].set(ylabel="SDD")

# B05 LOG(SDD)
lm = LinearRegression().fit(B05.values.reshape(-1,1), log_SDD)
ax[1,1].scatter(B05, log_SDD, color="#9D50A6", alpha=0.7)
ax[1,1].plot([min(B05),max(B05)],[lm.predict([[min(B05)]]),lm.predict([[max(B05)]])], color='gray', linestyle='--')
ax[1,1].set(ylabel="ln(SDD)")
ax[1,1].text(0.01,0.02,f'ln(SDD) = {round(lm.coef_[0],2)} • B05 + {round(lm.intercept_,2)}', transform=ax[1,1].transAxes)
ax[1,1].text(0.01,0.09,f'R² = {round(r2_score(log_SDD,lm.predict(B05.values.reshape(-1,1))),2)}', transform=ax[1,1].transAxes)

# B06
ax[2,0].scatter(B06, SDD, color="#17A77E", alpha=0.7)
ax[2,0].plot([min(B06),max(B06)],[max(SDD),min(SDD)], color='gray', linestyle='--')
ax[2,0].set(ylabel="SDD")

# B06 LOG(SDD)
lm = LinearRegression().fit(B06.values.reshape(-1,1), log_SDD)
ax[2,1].scatter(B06, log_SDD, color="#9D50A6", alpha=0.7)
ax[2,1].plot([min(B06),max(B06)],[lm.predict([[min(B06)]]),lm.predict([[max(B06)]])], color='gray', linestyle='--')
ax[2,1].set(ylabel="ln(SDD)")
ax[2,1].text(0.01,0.02,f'ln(SDD) = {round(lm.coef_[0],2)} • B06 + {round(lm.intercept_,2)}', transform=ax[2,1].transAxes)
ax[2,1].text(0.01,0.09,f'R² = {round(r2_score(log_SDD,lm.predict(B06.values.reshape(-1,1))),2)}', transform=ax[2,1].transAxes)

plt.tight_layout()
plt.subplots_adjust(wspace=0.2)
plt.show()

```

## Desarrollo del modelo

```{python}
#| echo: false
import sys
sys.path.append("scripts")  # only if your .py is not in the same folder
from regresion import *

X = df.drop(columns='valor')
y = df['valor']
```

Para el desarrollo del modelo se implementó la clase ```RegresionLineal```, que contiene el código necesario para la construcción y selección de variables así como para la validación cruzada del modelo. La misma requiere unicamente de los arrays correspondientes a la variable objetivo y a las características con las que estimarla.

Generar un modelo incial sin la construcción de variables adicionales y sin alterar la variable objetivo resulta sencillo:
```{python}
Modelo = RegresionLineal(X,y)
Modelo.stepwise()
```

Los coeficientes resultan:

```{python}
Modelo.ajustar_modelo_final()
```

Mientras que la correlación se visualiza de la siguiente manera:

```{python}
Modelo.graficar()
```

Esta correlación no es necesariamente mala, pero puede mejorarse mediante la implementación de las propuestas ya estudiadas como lo son la generación de cocientes entre características y la linealización de la variable objetivo:

```{python}
Modelo.stepwise(log = True, extendido = True)
```

```{python}
#| echo: false
#| output: false
Modelo.ajustar_modelo_final(log = True, extendido = True)
```

```{python}
Modelo.graficar(log = True, extendido = True)
```

Observamos que el modelo obtiene una notable mejoría con el uso de sólo cocientes como variables predictivas. Resta entonces, probar utilizar una cantidad mayor o menor de variables y analizar cómo esto afecta a la predicción.

```{python}
Modelo.stepwise(log = True, extendido = True, max_vars = 3)
```
```{python}
#| echo: false
#| output: false
Modelo.ajustar_modelo_final(log = True, extendido = True)
```

```{python}
#| echo: false
Modelo.graficar(log = True, extendido = True)
```