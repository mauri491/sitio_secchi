[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "",
    "text": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital"
  },
  {
    "objectID": "index.html#profundidad-de-disco-de-secchi",
    "href": "index.html#profundidad-de-disco-de-secchi",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "1.1 Profundidad de disco de Secchi",
    "text": "1.1 Profundidad de disco de Secchi\nLa profundidad de visibilidad del disco de Secchi \\((SDD)\\) es una medida ampliamente utilizada para evaluar la transparencia del agua. Se define como la profundidad en metros a la cual un disco blanco circular desaparece al dejarlo descender en un cuerpo de agua.\nEsta variable es un indicador indirecto de la calidad del agua, ya que se relaciona con la concentración de sedimentos en suspensión, fitoplancton y otros materiales particulados. Continua siendo utilizado debido a la simplicidad y rango de aplicación universal del método, además de tratarse de un parámetro fácilmente entendible por el público."
  },
  {
    "objectID": "index.html#métodos-tradicionales-de-estimación",
    "href": "index.html#métodos-tradicionales-de-estimación",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "1.2 Métodos tradicionales de estimación",
    "text": "1.2 Métodos tradicionales de estimación\nEn las últimas décadas, se han desarrollado múltiples ecuaciones empíricas y modelos basados en datos in situ y sensores remotos para estimar esta profundidad de manera eficiente y a gran escala. El objetivo de recopilar es identificar patrones comunes, variables predictoras relevantes y estrategias metodológicas empleadas.\n\n\nTabla 1: Algoritmos publicados para la predicción de \\(\\mathrm{SDD}\\) utilizando la plataforma Landstat [1].\n\n\n\n\n\n\n\n\n\nNombre\nVariable\nFormula\nMuestras\nR²\n\n\n\n\nDekker and Peters\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{ln(Red)}\\)\n15\n0.86\n\n\nDominguez Gomez et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{(Green)^x}\\)\n16\n0.90\n\n\nGiardino et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{Blue/Green}\\)\n4\n0.85\n\n\nKloiber et al.\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{Blue/Red + Blue}\\)\n374\n0.93\n\n\nLathrop and Lillesand\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{Green}\\)\n9\n0.98\n\n\nMancino et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{Red/Green + Blue/Green + Blue}\\)\n60\n0.82\n\n\n\n\nEn la mayoría de los casos la relación entre \\(SDD\\) y la intensidad de la luz es no lineal, por lo que se utiliza \\(ln(SDD)\\) para realizar la regresión.\nLa correlación con la banda roja puede explicarse causalmente por la correlación positiva directa entre la reflectancia en el rojo y la carga bruta de partículas que induce la dispersión de partículas. De manera que mientras que la claridad del agua \\((SDD)\\) desciende, la intensidad en el rojo aumenta [2].\nComo referencia se presentan las propiedades de las distintas bandas de la plataforma espacial Sentinel-2:\n\n\nTabla 2: Propiedades de las bandas S2-MSI, para las plataformas S2A y S2B.\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2A\n\nSentinel-2B\n\n\n\n\n\nBanda\nResolución espacial (m)\nLongitud de onda (nm)\nAncho de banda (nm)\nLongitud de onda (nm)\nAncho de banda (nm)\n\n\n\\(\\mathrm{B01}\\) (aerosol)\n60\n442.7\n20\n442.3\n20\n\n\n\\(\\mathrm{B02}\\) (blue)\n10\n492.7\n65\n492.3\n65\n\n\n\\(\\mathrm{B03}\\) (green)\n10\n559.8\n35\n558.9\n35\n\n\n\\(\\mathrm{B04}\\) (red)\n10\n664.6\n38\n664.9\n31\n\n\n\\(\\mathrm{B05}\\) (red edge)\n20\n794.1\n14\n703.8\n15\n\n\n\\(\\mathrm{B06}\\)\n20\n748.5\n14\n739.1\n13\n\n\n\\(\\mathrm{B07}\\)\n20\n782.8\n19\n779.7\n19\n\n\n\\(\\mathrm{B08}\\) (NIR)\n10\n832.8\n105\n832.9\n104\n\n\n\\(\\mathrm{B8A}\\)\n20\n864.7\n21\n864.0\n21\n\n\n\\(\\mathrm{B09}\\)\n60\n945.1\n19\n943.2\n20\n\n\n\\(\\mathrm{B10}\\)\n60\n1373.5\n29\n1376.9\n29\n\n\n\\(\\mathrm{B11}\\) (SWIR 1)\n20\n1613.7\n90\n1616.4\n94\n\n\n\\(\\mathrm{B12}\\) (SWIR 2)\n20\n2292.4\n174\n2185.7\n184"
  },
  {
    "objectID": "index.html#regresión-lineal",
    "href": "index.html#regresión-lineal",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "1.3 Regresión lineal",
    "text": "1.3 Regresión lineal\nEn estadística, la regresión lineal es un modelo matemático usado para aproximar la relación de dependencia entre una variable dependiente \\(Y\\) con \\(m\\) variables independientes \\(X_i\\). Este modelo puede ser expresado como:\n\\[ Y = \\beta_0 +  \\beta_1 X_1 + \\cdots + \\beta_m X_m\\]\ndonde:\n\n\\(Y\\) es la variable dependiente o variable de respuesta.\n\\(X_1,X_2,...X_m\\) son las variables explicativas, independientes o regresoras.\n\\(\\beta_0,\\beta_1,\\beta_2,...\\beta_m\\) son los parámetros del modelo, miden la influencia que las variables explicativas tienen sobre el regrediendo.\n\n\n1.3.1 Selección de variables\nLa seleccion de variables y características es el foco de mucha investigación. El objetivo de la selección de variables es mejorar el rendimiento de predicción de los predictores y proporcionar una mejor comprensión del proceso subyacente que generó los datos.\nLa identificación de las variables más relevantes para incluir o excluir en un modelo predictivo es un paso fundamental en cualquier investigación rigurosa, especialmente cuando se busca construir modelos con alto poder explicativo y capacidad de generalización.\nComprender qué variables contribuyen en mayor medida a la calidad de las predicciones permite no solo mejorar el desempeño del modelo, sino también interpretar con mayor claridad los fenómenos que se están estudiando.\nNo existe una forma segura de definir la importancia de una variable puesto que la utilidad de la misma depende del modelo implementado y de las demás variables con las que interactua: una variable que es completamente inutil por si misma puede resultar en una mejora del rendimiento significativa cuando es considerada junto con otras variables [3].\nA pesar de las dificultades existen estrategias útiles para determinar un subconjunto de variables útiles para la predicción. Tal es el caso de los métodos forward selection y backwards elimination. En forward selection, las variables son progresivamente incorporadas, evaluando el modelo al paso de cada una, mientras que en backwards elimination uno empieza con el conjunto entero de variables y progresivamente elimina las menos prometedoras.\nUna forma de implementación de forward selection es por empezar por un modelo exento de variables independientes, pero cuyo término independiente sea la media de las variables de respuesta. A partir de este valor se calcula el residuo como la diferencia entre el valor verdadero \\(Y\\) y el valor predecido \\(Y_{pred}\\).\n\\[r = Y - Y_{pred}\\]\nSeguidamente, se computa la correlación entre cada variable independiente \\(X_i\\) con el residuo y se incorpora al modelo la variable con la mayor correlación absoluta. El proceso se repite esta vez con las predicciones del nuevo modelo y se continua hasta alcanzar una desempeño deseado o un número determinado de variables. Este método fue utilizado satisfactoriamente en investigaciones previas [4].\n\n\n\nFig. 1: Matriz de correlación entre las distintas bandas.\n\n\nUna herramienta útil para ayudar en la selección de variables es una matriz de correlación. Dos variables prefectamente correlacionadas resultan redundantes en el sentido de que añadir ambas no aporta información adicional. Sin embargo, una correlación muy alta entre variables (o anti-correlación) no significa ausencia de complementariedad.\n\n\n1.3.2 Generación de características lineales y no lineales\nIncluir nuevas variables a partir de las originales, como productos, cocientes o transformaciones no lineales (por ejemplo, logaritmos, o raíces cuadradas) permite capturar relaciones más complejas entre las variables independientes y la variable objetivo. Por ejemplo, si se dispone de datos espectrales, pueden construirse razones entre bandas (band ratios) o índices espectrales que resalten ciertas propiedades físicas del fenómeno estudiado. Estas nuevas características permiten al modelo lineal aproximar mejor la relación entre los datos y la salida, especialmente cuando la relación real no es perfectamente lineal.\n\n\n1.3.3 Métricas de desempeño\nLas métricas de desempeño permiten cuantificar qué tan bien un modelo se ajusta a los datos y predice resultados. Algunas métricas evalúan la calidad del ajuste, otras penalizan la complejidad y algunas miden directamente el error de predicción.\nEl Coeficiente de Determinación Ajustado \\(R^2-ADJ\\) es una variante de R² que corrige la tendencia de este último a aumentar al añadir más variables al modelo, incluso si no aportan mejora real. Mientras que el R² tradicional mide la proporción de variabilidad de la variable dependiente explicada por el modelo, el ADJ-R²penaliza la inclusión de predictores irrelevantes, ajustando el valor según el número de observaciones y el número de variables independientes. Esto lo hace más útil para comparar modelos con distinto número de predictores, evitando la falsa impresión de mejora solo por complejidad adicional.\nEl Criterio de Información de Akaike (AIC) es una métrica utilizada para comparar modelos estadísticos en términos de ajuste y complejidad. Se basa en la teoría de la entropía y busca identificar el modelo que mejor explica los datos con el menor número posible de parámetros. Un valor de AIC más bajo indica un mejor equilibrio entre ajuste y simplicidad. La fórmula general es:\n\\[AIC = 2k - 2 ln(L)\\]\ndonde \\(k\\) es el número de parámetros estimados en el modelo y \\(L\\) es el valor máximo de la función de verosimilitud. A diferencia del ADJ-R² el AIC no tiene un valor máximo fijo y es más útil cuando se comparan múltiples modelos candidatos sobre el mismo conjunto de datos.\nEl Error Cuadrático Medio (MSE) mide el promedio de los cuadrados de las diferencias entre los valores observados y los predichos por el modelo. Es una métrica sensible a errores grandes, ya que los eleva al cuadrado, y por eso penaliza más las predicciones alejadas de la realidad. Su fórmula es:\n\\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\ndonde \\(y_i\\) y son \\(\\hat{y}_i\\) los valores reales y los valores predichos. Un MSE más bajo indica un mejor desempeño del modelo, pero su escala depende de la magnitud de la variable objetivo, por lo que no siempre es directamente interpretable sin contexto."
  },
  {
    "objectID": "index.html#índices-espectrales",
    "href": "index.html#índices-espectrales",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "1.4 Índices espectrales",
    "text": "1.4 Índices espectrales\nLos índices espectrales son combinaciones matemáticas de reflectancias en distintas bandas del espectro electromagnético obtenidas por sensores remotos, como satélites o drones. Estos índices realzan ciertas características o propiedades de la superficie observada, facilitando la identificación y monitoreo de elementos como vegetación, agua, suelo o áreas urbanas. Al reducir la información a una sola variable combinada, permiten detectar cambios y evaluar condiciones ambientales con mayor precisión que el análisis de bandas individuales.\n\n1.4.1 NDWI\nEl Índice de Diferencia Normalizada del Agua (NDWI) es un índice espectral diseñado para resaltar la presencia y extensión de cuerpos de agua en imágenes satelitales. Se calcula utilizando reflectancias en bandas específicas del verde y el infrarrojo cercano, y su fórmula clásica es:\n\\[NDWI = \\frac{R_{\\text{verde}} - R_{\\text{NIR}}}{R_{\\text{verde}} + R_{\\text{NIR}}}\\]\ndonde $R_{verde} es la reflectancia en la banda verde y \\(R_{NIR}\\) la reflectancia en la banda del infrarrojo cercano. Valores altos de NDWI indican presencia probable de agua, ya que el agua absorbe fuertemente en el NIR y refleja más en el verde."
  },
  {
    "objectID": "index.html#librerías",
    "href": "index.html#librerías",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "2.1 Librerías",
    "text": "2.1 Librerías\nPara regresión lineal se usan principalmente las librerías de Python numpy, pandas y Scikit-learn.\n\nScikit-learn: está en el corazón de las operaciones de ciencias de datos en Python. Ofrece módulos para procesamiento de datos, aprendizaje supervisado y no supervisado, selección y validación de modelos, y métricas de error.\nPandas: especializada en la manipulación y el análisis de datos. Ofrece estructuras de datos y operaciones para manipular tablas numéricas y series temporales.\nNumPy: provee al usuario con arreglos multidimensionales, junto a un gran conjunto de funciones para operar en estos."
  },
  {
    "objectID": "index.html#lectura-de-datos",
    "href": "index.html#lectura-de-datos",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "2.2 Lectura de datos",
    "text": "2.2 Lectura de datos\nPara la carga, lectura y manipulación de la información la librería Pandas permite convertir un archivo .csv en un DataFrame, estructura de datos que facilita la manipulación de estos. El fragmento de código siguiente agrupa los valores de \\(SDD\\) con los valores de reflectancia a diferentes longitudes de onda para los cuales coinciden fecha, latitud y longitud y crea un nuevo DataFrame.\n\nimport pandas as pd\n\n# Rutas a los archivos CSV\narchivo_reflectancias = \"datos\\\\base_de_datos_gis.csv\"  # contiene: fecha,punto,pixel,banda,reflect,longitud,latitud\narchivo_parametros = \"datos\\\\base_de_datos_lab.csv\"     # contiene: fecha,longitud,latitud,param,valor\n\n# Leer los archivos\ndf_reflect = pd.read_csv(archivo_reflectancias)\ndf_param = pd.read_csv(archivo_parametros)\n\n# Filtrar los parámetros \"secchi\"\ndf_secchi = df_param[df_param[\"param\"].str.lower() == \"secchi\"]\n\n# Merge por fecha y coordenadas\nmerged = pd.merge(\n    df_secchi,\n    df_reflect,\n    on=[\"fecha\", \"latitud\", \"longitud\"],\n    how=\"inner\"\n)\n\n# Pivotear la tabla para poner bandas como columnas\ntabla_final = merged.pivot_table(\n    index=[\"param\", \"fecha\", \"longitud\", \"latitud\", \"valor\"], \n    columns=\"banda\",\n    values=\"reflect\"\n).reset_index()\n\n# Reordenar columnas: param | B01 | B02 | ... | B8A\nbandas = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12', 'B8A']\ncolumnas_finales = ['valor'] + bandas\n\n# Crear Tabla final\ndf = tabla_final[columnas_finales]\n\n# Guardar excel para pruebas finales\nsalida_excel = \".//datos//tabla_final.xlsx\"\ndf.to_excel(salida_excel, index=False)\n\nEl DataFrame obtenido se asemeja a la siguiente tabla:\n\n\nTabla 3: DataFrame de los datos recolectados.\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{B01}\\)\n\\(\\mathrm{B02}\\)\n\\(\\cdots\\)\n\\(\\mathrm{B12}\\)\n\\(\\mathrm{B8A}\\)\n\n\n\n\n10\n0,1728\n0,1754\n\\(\\cdots\\)\n0,1404\n0,1869\n\n\n15\n0,1497\n0,17022\n\\(\\cdots\\)\n0,1113\n0,1567\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\cdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n125\n0,1571\n0,1563\n\\(\\cdots\\)\n0,1419\n0,1436\n\n\n135\n0,1503\n0,1591\n\\(\\cdots\\)\n0,1420\n0,1454"
  },
  {
    "objectID": "index.html#análisis-de-los-datos",
    "href": "index.html#análisis-de-los-datos",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "2.3 Análisis de los datos",
    "text": "2.3 Análisis de los datos\nLa relación no lineal entre la penetración de la luz y la profundidad de disco de Secchi fue ya descrita por diversos autores que hallan una mejor descripción de esta como una de tipo logaritmica [5]. Se verifica este comportamiento con las bandas B04, B05 y B06:"
  },
  {
    "objectID": "index.html#desarrollo-del-modelo",
    "href": "index.html#desarrollo-del-modelo",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "2.4 Desarrollo del modelo",
    "text": "2.4 Desarrollo del modelo\nPara el desarrollo del modelo se implementó la clase RegresionLineal, que contiene el código necesario para la construcción y selección de variables así como para la validación cruzada del modelo. La misma requiere unicamente de los arrays correspondientes a la variable objetivo y a las características con las que estimarla. El script se encuentra disponible acá.\nSe realiza la búsqueda de ecuaciones utilizando los valores de reflectancia corregidos por SEN2COR y por ACOLITE de manera separada.\n\n2.4.1 Corrección por SEN2COR\nLa implementación del método sepwise forward selection permite evaluar la adición de variables al modelo predictivo en una instancia inicial donde no se general características adicionales. Los resultados de cada paso se encuentran resumidos en la siguiente tabla:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.0263\n-0.0263\n44.7372\n119.03\n\n\n1\nB05\n0.2761\n0.2221\n37.6319\n113.56\n\n\n2\nB02\n0.7158\n0.6701\n23.2318\n99.69\n\n\n3\nB08\n0.7614\n0.6989\n21.3544\n99.41\n\n\n4\nB03\n0.8165\n0.7459\n18.7613\n97.38\n\n\n5\nB04\n0.8296\n0.7383\n17.9258\n97.47\n\n\n\n\n\nLa combinación de B05, B02 y B08 resulta en el mejor ajuste para valores mínimos de RMSE y AIC.\nHaciendo uso de la relación logaritmica entre la profundidad de disco y la penetración de la luz descrita y probada previamente se obtiene un modelo con una notable mejoría:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nB05\n0.2949\n0.2421\n36.9801\n112.79\n\n\n2\nB02\n0.8333\n0.8063\n17.7159\n91.89\n\n\n3\nB08\n0.8379\n0.7950\n17.4478\n93.32\n\n\n4\nB11\n0.8519\n0.7944\n16.5047\n93.56\n\n\n5\nB07\n0.8549\n0.7768\n16.5084\n95.74\n\n\n\n\n\nLa mejoría se extiende hasta la adición de la variable B07, después de la cual hay una ligera disminución de R²-ADJ, pero no lo suficientemente significativa como para tratarse de un caso de sobreajuste considerable.\nLa aplicación del logaritmo puede extenderse a las variables mismas, resultando en un modelo predictivo mucho más certero con una menor cantidad de variables:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nlog(B05)\n0.4339\n0.3916\n32.8131\n108.64\n\n\n2\nlog(B02)\n0.8834\n0.8644\n15.0118\n87.21\n\n\n3\nB06\n0.8934\n0.8651\n14.3537\n87.65\n\n\n4\nB08\n0.9014\n0.8631\n13.6956\n88.17\n\n\n5\nB12\n0.9093\n0.8606\n13.0846\n88.57\n\n\n\n\n\nLa combinación de log(B05) y B02 resulta prometedora: el valor de AIC es mínimo, el RMSE es bajo, y el R² alto. Los coeficientes y ordenada de la ecuación resultante son:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nlog(B05)\n0.4339\n0.3916\n32.8131\n108.64\n\n\n2\nlog(B02)\n0.8834\n0.8644\n15.0118\n87.21\n\n\n\n\n\nLos coeficientes y ordenada resultantes de considerar a las primeras dos variables son:\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\nlog(B05)\n-12.260673\n\n\nlog(B02)\n13.030511\n\n\nOrdenada\n5.275337\n\n\n\n\n\nEl uso de cocientes entre las variables otorga una cantidad mucho más extensiva de predictores, observamos que de considerar a estos junto con las variables linealizadas, estos predominan como mejores contribuyentes a la mejora de la predicción:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nB05/B02\n0.8822\n0.8734\n15.0707\n84.77\n\n\n2\nlog(B01)\n0.8966\n0.8798\n14.0689\n84.36\n\n\n3\nB08/B06\n0.9091\n0.8850\n13.1732\n84.66\n\n\n4\nB8A/B11\n0.9100\n0.8750\n13.0816\n86.29\n\n\n5\nB07/B11\n0.9087\n0.8596\n13.1518\n88.25\n\n\n\n\n\nLos coeficientes y ordenada resultantes de considerar a las primeras dos variables son:\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\nB05/B02\n-4.268558\n\n\nlog(B01)\n0.643369\n\n\nOrdenada\n9.430494\n\n\n\n\n\n\n\n2.4.2 Corrección por ACOLITE\nSe repite el proceso que con los datos corregidos por el algoritmo SEN2COR. Se menciona que en la lectura y tratamiento de datos se descartaron los puntos para los cuales los valores de reflectancia eran negativos luego de la corrección.\nPartimos de haber hallado que la aplicación del logaritmo a la profundidad de disco resulta en un mejor ajuste. En relación a los valores corregidos por ACOLITE esto devuelve los siguientes resultados:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.2303\n-0.2303\n47.8282\n59.46\n\n\n1\nB05\n0.1189\n-0.0378\n38.8925\n56.79\n\n\n2\nB02\n0.6160\n0.4521\n27.3299\n51.54\n\n\n3\nB08\n0.7201\n0.4925\n23.1573\n51.70\n\n\n4\nB03\n0.8162\n0.5245\n16.6068\n48.02\n\n\n5\nB06\n0.8334\n0.2594\n16.2201\n49.84\n\n\n\n\n\nObservamos que para las mismas combinaciones los valores de R² y R²-ADJ aumentaron significativamente respecto a aquellos con la corrección por SEN2COR.\nPara una relación doblemente logarítimica, obtenemos lo siguiente:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.2303\n-0.2303\n47.8282\n59.46\n\n\n1\nlog(B05)\n0.3287\n0.2083\n28.9200\n52.62\n\n\n2\nlog(B02)\n0.6888\n0.5550\n21.0484\n49.21\n\n\n3\nB01\n0.7729\n0.5845\n18.4856\n48.91\n\n\n4\nlog(B08)\n0.8185\n0.5269\n15.6606\n48.45\n\n\n5\nB06\n0.8315\n0.2347\n15.0506\n50.27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\n0\nlog(B05)\n-5.058134\n\n\n1\nlog(B02)\n6.861923\n\n\n2\nB01\n-18.872053\n\n\n3\nOrdenada\n7.577205\n\n\n\n\n\n\n\nDonde esta vez se ve una desmejora respecto al modelo corregido por SEN2COR para un modelo de dos variables logarítimicas.\nLa incorporación de cocientes al abanico de variables devuelve las siguientes métricas:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.2303\n-0.2303\n47.8282\n59.46\n\n\n1\nB04/B02\n0.7325\n0.6853\n20.3765\n47.21\n\n\n2\nB04/B01\n0.7795\n0.6848\n19.1909\n48.01\n\n\n3\nB07/B06\n0.8216\n0.6737\n15.8882\n47.67\n\n\n4\nB08\n0.8167\n0.5294\n16.0595\n49.33\n\n\n5\nB11/B05\n0.8592\n0.3978\n13.5437\n48.60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\n0\nB04/B02\n-1.582140\n\n\n1\nOrdenada\n6.402011\n\n\n\n\n\n\n\nNuevamente, se ve una desmejora respecto al modelo corregido por SEN2COR. El número de variables se reduce a una."
  },
  {
    "objectID": "index.html#ecuaciones-halladas",
    "href": "index.html#ecuaciones-halladas",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "2.5 Ecuaciones halladas",
    "text": "2.5 Ecuaciones halladas\nEl par de mejores ecuaciones halladas para cada algoritmo de corrección utilizando la transformación logarítmica fueron las siguientes: \\[ log(SDD)_{SEN2COR} = -12.26 \\cdot log(B05) + 13.03 \\cdot log(B02) + 5.27 \\]\n\\[ log(SDD)_{ACOLITE} = -5.06 \\cdot log(B05) + 6.86 \\cdot log(B02) -18.87 \\cdot B01 \\]\nSus ajustes se visualizan de la siguiente manera:\n\n\n\n\n\n\n\n\n\nEl par de mejores ecuaciones halladas para cada algoritmo utilizando cocientes entre las variables es:\n\\[ log(SDD)_{SEN2COR} = -4.27\\frac{B05}{B02} + 0.64\\cdot log(B01)+ 9.43 \\]\n\\[ log(SDD)_{ACOLITE} = -1.58\\frac{B04}{B02} + 6.40 \\]\nSus ajustes se visualizan de la siguiente manera:\n\n\n\n\n\n\n\n\n\nObservamos que la utilización del algoritmo de ACOLITE para la corrección atmosférica ocasiona una desmejora en la estimación."
  },
  {
    "objectID": "index.html#mapas-de-la-estimación",
    "href": "index.html#mapas-de-la-estimación",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "2.6 Mapas de la estimación",
    "text": "2.6 Mapas de la estimación\nLos mapas generados para cada fecha corresponden al recorte del área de interés del producto satelital corregido por ACOLITE.\n\n06/09/2516/12/2430/05/2420/05/2412/12/2311/05/23"
  },
  {
    "objectID": "index.html#random-forest",
    "href": "index.html#random-forest",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "3.1 Random Forest",
    "text": "3.1 Random Forest\nRandom Forest es un algoritmo de aprendizaje automático basado en un conjunto de árboles de decisión entrenados con muestras y variables aleatorias. La combinación de múltiples árboles reduce la varianza y mejora la capacidad de generalización del modelo. Es robusto frente al sobreajuste y adecuado para capturar relaciones no lineales entre las variables.\n\n3.1.1 Hiperparámetros\nLos hiperparámetros son valores que se definen antes del entrenamiento de un modelo y controlan cómo aprende. Ajustarlos correctamente permite mejorar el rendimiento y la capacidad de generalización del modelo.\n\nn_estimators es el número total de árboles que se crearán en el bosque aleatorio. Cada árbol se entrena con una muestra distinta de los datos (si bootstrap=True) y sus predicciones se promedian. Aumentar este número hace que el modelo sea más estable y consistente, reduciendo la varianza y mejorando la precisión. Sin embargo, también incrementa el tiempo de entrenamiento y el uso de memoria, por lo que hay que buscar un equilibrio.\nmax_depth determina la cantidad máxima de niveles que puede tener cada árbol, es decir, cuántas divisiones sucesivas puede hacer desde la raíz hasta una hoja. Cuanto más profundo sea un árbol, más patrones complejos puede aprender, pero también corre el riesgo de memorizar el ruido de los datos (sobreajuste). Por el contrario, limitar la profundidad hace que los árboles sean más simples y generalicen mejor, aunque podrían no capturar todas las relaciones presentes en los datos.\nmin_samples_split indica el número mínimo de muestras necesarias para dividir un nodo en dos ramas. Este parámetro evita que el árbol realice divisiones basadas en muy pocos datos, que podrían ser poco representativos. Valores pequeños permiten divisiones frecuentes, generando árboles más complejos y propensos al sobreajuste, mientras que valores mayores producen divisiones más conservadoras y árboles más suaves que generalizan mejor.\nmin_samples_leaf establece el mínimo de muestras que debe contener una hoja, es decir, un nodo terminal del árbol. Si se permite una sola muestra por hoja, el árbol puede memorizar los datos y generar predicciones muy específicas pero poco generalizables. Aumentar este valor hace que cada hoja represente un promedio de varias observaciones, produciendo predicciones más estables y menos sensibles al ruido.\nmax_features determina cuántas variables se consideran al buscar la mejor división en cada nodo. Limitar la cantidad de features introduce aleatoriedad en cada árbol, lo que reduce la correlación entre ellos y mejora la generalización del bosque. Por ejemplo, “sqrt” utiliza la raíz cuadrada del total de variables y “log2” utiliza el logaritmo base 2 del total de variables. La diferencia es sutil pero importante: “log2” suele considerar menos variables en cada división, aumentando la diversidad entre árboles, mientras que “sqrt” toma un número ligeramente mayor, equilibrando diversidad y cantidad de información.\nbootstrap indica si cada árbol se entrena con una muestra aleatoria con reemplazo del conjunto total de datos. Si es True, cada árbol ve datos diferentes, lo que genera más diversidad entre árboles y mejora la capacidad de generalización. Si es False, todos los árboles utilizan el mismo conjunto, lo que puede hacerlos más parecidos entre sí y aumentar la posibilidad de sobreajuste. Además, usar bootstrap permite calcular errores de tipo “out-of-bag” para evaluar el modelo sin necesidad de un conjunto de validación separado.\n\n\n\n3.1.2 Resultados\n\nEl modelo Random Forest mostró un buen nivel de ajuste y capacidad predictiva, capturando adecuadamente las relaciones entre variables. Sin embargo, presentó un RMSE más alto en comparación con los modelos lineales, lo que indica mayor dispersión en las predicciones y una menor precisión promedio pese a su buen desempeño general. La importancia de cada variable fue la siguiente:\n\nLas bandas B04 y B04 predominan en el ranking, esto puede deberse a que las bandas Red y Red Edge (B05–B06-B07) se ubican justo en la transición entre el rojo y el infrarrojo cercano, donde la reflectancia del agua cambia mucho con la concentración de sedimentos. Esto las hace excelentes indicadores de la turbidez y material particulado suspendido.\nCon el objetivo de mejorar el desempeño de los modelos, se incorporaron cocientes entre variables como nuevas características predictoras. Estas relaciones permiten resaltar contrastes espectrales y capturar patrones relativos entre bandas, lo que puede aumentar la sensibilidad del modelo frente a variaciones en la transparencia del agua y mejorar la estimación.\n\nAl introducir los cocientes entre variables como nuevas características, se observa una mejora notable en el desempeño de los modelos. En particular, el RMSE se redujo hasta menos de la mitad, indicando predicciones más precisas y consistentes"
  },
  {
    "objectID": "index.html#xgboost",
    "href": "index.html#xgboost",
    "title": "Estimación de profundidad de disco de Secchi para el desarrollo de un algoritmo mediante técnicas de teledetección satelital",
    "section": "3.2 XGBoost",
    "text": "3.2 XGBoost\nXGBoost (Extreme Gradient Boosting) es un método de boosting que construye árboles de manera secuencial, donde cada uno corrige los errores del anterior. Incorpora técnicas de regularización y optimización que aumentan la precisión y evitan el sobreajuste, ofreciendo un excelente desempeño en tareas de regresión y clasificación.\n\n3.2.1 Hiperparámetros\nAdemás de los hiperparámetros n_estimators y max_depth ya vistos para Random Forest, XGBoost cuenta con los siguientes hiperparámetros:\n\nlearning_rate controla cuánto contribuye cada árbol nuevo a la predicción total. Valores bajos (por ejemplo, 0.05) hacen que el modelo aprenda más despacio y con mayor precisión, requiriendo más árboles (n_estimators) para converger. Valores altos aceleran el aprendizaje, pero pueden provocar sobreajuste si los árboles son profundos o abundantes.\nsubsample indica la fracción de muestras de entrenamiento que se usan para construir cada árbol. Un valor menor que 1 introduce aleatoriedad, lo que reduce la correlación entre árboles y mejora la generalización, pero demasiado bajo puede hacer que el modelo no aprenda bien. Un valor de 1 significa que todos los datos se usan en cada árbol, maximizando ajuste pero con menor diversidad.\ncolsample_bytree define la fracción de variables (features) consideradas al entrenar cada árbol. Limitar las columnas aumenta la aleatoriedad entre árboles, reduciendo la correlación y el sobreajuste. Por ejemplo, 0.8 significa que cada árbol ve solo el 80% de las variables disponibles.\ngamma es el mínimo incremento en la función de pérdida requerido para realizar una división adicional en un nodo. Valores mayores hacen que el árbol sea más conservador, evitando divisiones que aporten poca mejora en la predicción, lo que ayuda a reducir sobreajuste.\nreg_alpha es la penalización de tipo L1 sobre los pesos de los árboles. Ayuda a reducir la complejidad del modelo y puede forzar algunos pesos a cero, promoviendo sparsity y simplificación. Esto es útil si hay muchas variables poco relevantes.\nreg_lambda es la penalización de tipo L2 sobre los pesos de los árboles. Contrarresta valores extremos y ayuda a estabilizar el modelo, evitando que algunos árboles dominen la predicción y reduciendo el riesgo de sobreajuste.\n\n\n\n3.2.2 Resultados\n\nEl modelo XGBoost demuestra desde un inicio un poder predictivo mayor al de Random Forest y a su vez una dispersión mucho menor en las predicciones.\n\nNuevamente se repite la predominancia de las bandas B04 y B05 en el ranking.\n\nSe observa que la incorporación de cocientes incrementa considerablemente el poder predictivo, a la vez que el RMSE disminuye a valor comparables con el mejor de los valores lineales previamente obtenidos."
  },
  {
    "objectID": "modelos_ml/modelos.html",
    "href": "modelos_ml/modelos.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\n\n# Rutas a los archivos CSV\narchivo_reflectancias = \"..\\\\datos\\\\base_de_datos_gis.csv\"  # contiene: fecha,punto,pixel,banda,reflect,longitud,latitud\narchivo_parametros = \"..\\\\datos\\\\base_de_datos_lab.csv\"     # contiene: fecha,longitud,latitud,param,valor\n\n# Leer los archivos\ndf_reflect = pd.read_csv(archivo_reflectancias)\ndf_param = pd.read_csv(archivo_parametros)\n\n# Filtrar los parámetros \"secchi\"\ndf_secchi = df_param[df_param[\"param\"].str.lower() == \"secchi\"]\n\n# Merge por fecha y coordenadas\nmerged = pd.merge(\n    df_secchi,\n    df_reflect,\n    on=[\"fecha\", \"latitud\", \"longitud\"],\n    how=\"inner\"\n)\n\n# Pivotear la tabla para poner bandas como columnas\ntabla_final = merged.pivot_table(\n    index=[\"param\", \"fecha\", \"longitud\", \"latitud\", \"valor\"], \n    columns=\"banda\",\n    values=\"reflect\"\n).reset_index()\n\n# Reordenar columnas: param | B01 | B02 | ... | B8A\nbandas = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12', 'B8A']\ncolumnas_finales = ['valor'] + bandas\n\n# Crear Tabla final\ndf = tabla_final[columnas_finales]\nX = df.drop(columns='valor')\ny = df['valor']\n\n\nimport sys\nsys.path.append(\"..//scripts\")  # only if your .py is not in the same folder\nfrom machine_learning import *\n\n\nModelo = RegresionML(X,y, ratios = True)\nModelo.train_xgb()\nModelo.graficar(model = \"xgb\")\nModelo.graficar_importancia(model=\"xgb\", top_n=11)\n\nFitting 3 folds for each of 864 candidates, totalling 2592 fits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pickle\n\nwith open('C://Users//mauri//OneDrive//Desktop//PARANA//sitio_secchi//modelos_ml//rf_251014_0.9423_0.8823.pkl', 'rb') as f:\n    data = pickle.load(f)\n\nprint(type(data))\nprint(data)\n\n&lt;class 'numpy.ndarray'&gt;\n['B01' 'B02' 'B03' 'B04' 'B05' 'B06' 'B07' 'B08' 'B11' 'B12' 'B8A']"
  },
  {
    "objectID": "desarrollo.html",
    "href": "desarrollo.html",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "",
    "text": "Esta revisión tiene como objetivo identificar los enfoques más comunes en la literatura, los tipos de datos utilizados y las variables espectrales que se consideran más relevantes para este tipo de estimaciones.\n\n\nLa profundidad de visibilidad del disco de Secchi \\((SDD)\\) es una medida ampliamente utilizada para evaluar la transparencia del agua. Se define como la profundidad en metros a la cual un disco blanco circular desaparece al dejarlo descender en un cuerpo de agua.\nEsta variable es un indicador indirecto de la calidad del agua, ya que se relaciona con la concentración de sedimentos en suspensión, fitoplancton y otros materiales particulados. Continua siendo utilizado debido a la simplicidad y rango de aplicación universal del método, además de tratarse de un parámetro fácilmente entendible por el público.\n\n\n\nEn las últimas décadas, se han desarrollado múltiples ecuaciones empíricas y modelos basados en datos in situ y sensores remotos para estimar esta profundidad de manera eficiente y a gran escala. El objetivo de recopilar es identificar patrones comunes, variables predictoras relevantes y estrategias metodológicas empleadas.\n\n\nTabla 1: Algoritmos publicados para la predicción de \\(\\mathrm{SDD}\\) utilizando la plataforma Landstat [1].\n\n\n\n\n\n\n\n\n\nNombre\nVariable\nFormula\nMuestras\nR²\n\n\n\n\nDekker and Peters\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{ln(Red)}\\)\n15\n0.86\n\n\nDominguez Gomez et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{(Green)^x}\\)\n16\n0.90\n\n\nGiardino et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{Blue/Green}\\)\n4\n0.85\n\n\nKloiber et al.\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{Blue/Red + Blue}\\)\n374\n0.93\n\n\nLathrop and Lillesand\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{Green}\\)\n9\n0.98\n\n\nMancino et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{Red/Green + Blue/Green + Blue}\\)\n60\n0.82\n\n\n\n\nEn la mayoría de los casos la relación entre \\(SDD\\) y la intensidad de la luz es no lineal, por lo que se utiliza \\(ln(SDD)\\) para realizar la regresión.\nLa correlación con la banda roja puede explicarse causalmente por la correlación positiva directa entre la reflectancia en el rojo y la carga bruta de partículas que induce la dispersión de partículas. De manera que mientras que la claridad del agua \\((SDD)\\) desciende, la intensidad en el rojo aumenta [2].\nComo referencia se presentan las propiedades de las distintas bandas de la plataforma espacial Sentinel-2:\n\n\nTabla 2: Propiedades de las bandas S2-MSI, para las plataformas S2A y S2B.\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2A\n\nSentinel-2B\n\n\n\n\n\nBanda\nResolución espacial (m)\nLongitud de onda (nm)\nAncho de banda (nm)\nLongitud de onda (nm)\nAncho de banda (nm)\n\n\n\\(\\mathrm{B01}\\) (aerosol)\n60\n442.7\n20\n442.3\n20\n\n\n\\(\\mathrm{B02}\\) (blue)\n10\n492.7\n65\n492.3\n65\n\n\n\\(\\mathrm{B03}\\) (green)\n10\n559.8\n35\n558.9\n35\n\n\n\\(\\mathrm{B04}\\) (red)\n10\n664.6\n38\n664.9\n31\n\n\n\\(\\mathrm{B05}\\) (red edge)\n20\n794.1\n14\n703.8\n15\n\n\n\\(\\mathrm{B06}\\)\n20\n748.5\n14\n739.1\n13\n\n\n\\(\\mathrm{B07}\\)\n20\n782.8\n19\n779.7\n19\n\n\n\\(\\mathrm{B08}\\) (NIR)\n10\n832.8\n105\n832.9\n104\n\n\n\\(\\mathrm{B8A}\\)\n20\n864.7\n21\n864.0\n21\n\n\n\\(\\mathrm{B09}\\)\n60\n945.1\n19\n943.2\n20\n\n\n\\(\\mathrm{B10}\\)\n60\n1373.5\n29\n1376.9\n29\n\n\n\\(\\mathrm{B11}\\) (SWIR 1)\n20\n1613.7\n90\n1616.4\n94\n\n\n\\(\\mathrm{B12}\\) (SWIR 2)\n20\n2292.4\n174\n2185.7\n184\n\n\n\n\n\n\n\nEn estadística, la regresión lineal es un modelo matemático usado para aproximar la relación de dependencia entre una variable dependiente \\(Y\\) con \\(m\\) variables independientes \\(X_i\\). Este modelo puede ser expresado como:\n\\[ Y = \\beta_0 +  \\beta_1 X_1 + \\cdots + \\beta_m X_m\\]\ndonde:\n\n\\(Y\\) es la variable dependiente o variable de respuesta.\n\\(X_1,X_2,...X_m\\) son las variables explicativas, independientes o regresoras.\n\\(\\beta_0,\\beta_1,\\beta_2,...\\beta_m\\) son los parámetros del modelo, miden la influencia que las variables explicativas tienen sobre el regrediendo.\n\n\n\nLa seleccion de variables y características es el foco de mucha investigación. El objetivo de la selección de variables es mejorar el rendimiento de predicción de los predictores y proporcionar una mejor comprensión del proceso subyacente que generó los datos.\nLa identificación de las variables más relevantes para incluir o excluir en un modelo predictivo es un paso fundamental en cualquier investigación rigurosa, especialmente cuando se busca construir modelos con alto poder explicativo y capacidad de generalización.\nComprender qué variables contribuyen en mayor medida a la calidad de las predicciones permite no solo mejorar el desempeño del modelo, sino también interpretar con mayor claridad los fenómenos que se están estudiando.\nNo existe una forma segura de definir la importancia de una variable puesto que la utilidad de la misma depende del modelo implementado y de las demás variables con las que interactua: una variable que es completamente inutil por si misma puede resultar en una mejora del rendimiento significativa cuando es considerada junto con otras variables [3].\nA pesar de las dificultades existen estrategias útiles para determinar un subconjunto de variables útiles para la predicción. Tal es el caso de los métodos forward selection y backwards elimination. En forward selection, las variables son progresivamente incorporadas, evaluando el modelo al paso de cada una, mientras que en backwards elimination uno empieza con el conjunto entero de variables y progresivamente elimina las menos prometedoras.\nUna forma de implementación de forward selection es por empezar por un modelo exento de variables independientes, pero cuyo término independiente sea la media de las variables de respuesta. A partir de este valor se calcula el residuo como la diferencia entre el valor verdadero \\(Y\\) y el valor predecido \\(Y_{pred}\\).\n\\[r = Y - Y_{pred}\\]\nSeguidamente, se computa la correlación entre cada variable independiente \\(X_i\\) con el residuo y se incorpora al modelo la variable con la mayor correlación absoluta. El proceso se repite esta vez con las predicciones del nuevo modelo y se continua hasta alcanzar una desempeño deseado o un número determinado de variables. Este método fue utilizado satisfactoriamente en investigaciones previas [4].\n\n\n\nFig. 1: Matriz de correlación entre las distintas bandas.\n\n\nUna herramienta útil para ayudar en la selección de variables es una matriz de correlación. Dos variables prefectamente correlacionadas resultan redundantes en el sentido de que añadir ambas no aporta información adicional. Sin embargo, una correlación muy alta entre variables (o anti-correlación) no significa ausencia de complementariedad.\n\n\n\nIncluir nuevas variables a partir de las originales, como productos, cocientes o transformaciones no lineales (por ejemplo, logaritmos, o raíces cuadradas) permite capturar relaciones más complejas entre las variables independientes y la variable objetivo. Por ejemplo, si se dispone de datos espectrales, pueden construirse razones entre bandas (band ratios) o índices espectrales que resalten ciertas propiedades físicas del fenómeno estudiado. Estas nuevas características permiten al modelo lineal aproximar mejor la relación entre los datos y la salida, especialmente cuando la relación real no es perfectamente lineal.\n\n\n\nLas métricas de desempeño permiten cuantificar qué tan bien un modelo se ajusta a los datos y predice resultados. Algunas métricas evalúan la calidad del ajuste, otras penalizan la complejidad y algunas miden directamente el error de predicción.\nEl Coeficiente de Determinación Ajustado \\(R^2-ADJ\\) es una variante de R² que corrige la tendencia de este último a aumentar al añadir más variables al modelo, incluso si no aportan mejora real. Mientras que el R² tradicional mide la proporción de variabilidad de la variable dependiente explicada por el modelo, el ADJ-R²penaliza la inclusión de predictores irrelevantes, ajustando el valor según el número de observaciones y el número de variables independientes. Esto lo hace más útil para comparar modelos con distinto número de predictores, evitando la falsa impresión de mejora solo por complejidad adicional.\nEl Criterio de Información de Akaike (AIC) es una métrica utilizada para comparar modelos estadísticos en términos de ajuste y complejidad. Se basa en la teoría de la entropía y busca identificar el modelo que mejor explica los datos con el menor número posible de parámetros. Un valor de AIC más bajo indica un mejor equilibrio entre ajuste y simplicidad. La fórmula general es:\n\\[AIC = 2k - 2 ln(L)\\]\ndonde \\(k\\) es el número de parámetros estimados en el modelo y \\(L\\) es el valor máximo de la función de verosimilitud. A diferencia del ADJ-R² el AIC no tiene un valor máximo fijo y es más útil cuando se comparan múltiples modelos candidatos sobre el mismo conjunto de datos.\nEl Error Cuadrático Medio (MSE) mide el promedio de los cuadrados de las diferencias entre los valores observados y los predichos por el modelo. Es una métrica sensible a errores grandes, ya que los eleva al cuadrado, y por eso penaliza más las predicciones alejadas de la realidad. Su fórmula es:\n\\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\ndonde \\(y_i\\) y son \\(\\hat{y}_i\\) los valores reales y los valores predichos. Un MSE más bajo indica un mejor desempeño del modelo, pero su escala depende de la magnitud de la variable objetivo, por lo que no siempre es directamente interpretable sin contexto.\n\n\n\n\nLos índices espectrales son combinaciones matemáticas de reflectancias en distintas bandas del espectro electromagnético obtenidas por sensores remotos, como satélites o drones. Estos índices realzan ciertas características o propiedades de la superficie observada, facilitando la identificación y monitoreo de elementos como vegetación, agua, suelo o áreas urbanas. Al reducir la información a una sola variable combinada, permiten detectar cambios y evaluar condiciones ambientales con mayor precisión que el análisis de bandas individuales.\n\n\nEl Índice de Diferencia Normalizada del Agua (NDWI) es un índice espectral diseñado para resaltar la presencia y extensión de cuerpos de agua en imágenes satelitales. Se calcula utilizando reflectancias en bandas específicas del verde y el infrarrojo cercano, y su fórmula clásica es:\n\\[NDWI = \\frac{R_{\\text{verde}} - R_{\\text{NIR}}}{R_{\\text{verde}} + R_{\\text{NIR}}}\\]\ndonde $R_{verde} es la reflectancia en la banda verde y \\(R_{NIR}\\) la reflectancia en la banda del infrarrojo cercano. Valores altos de NDWI indican presencia probable de agua, ya que el agua absorbe fuertemente en el NIR y refleja más en el verde."
  },
  {
    "objectID": "desarrollo.html#profundidad-de-disco-de-secchi",
    "href": "desarrollo.html#profundidad-de-disco-de-secchi",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "",
    "text": "La profundidad de visibilidad del disco de Secchi \\((SDD)\\) es una medida ampliamente utilizada para evaluar la transparencia del agua. Se define como la profundidad en metros a la cual un disco blanco circular desaparece al dejarlo descender en un cuerpo de agua.\nEsta variable es un indicador indirecto de la calidad del agua, ya que se relaciona con la concentración de sedimentos en suspensión, fitoplancton y otros materiales particulados. Continua siendo utilizado debido a la simplicidad y rango de aplicación universal del método, además de tratarse de un parámetro fácilmente entendible por el público."
  },
  {
    "objectID": "desarrollo.html#métodos-tradicionales-de-estimación",
    "href": "desarrollo.html#métodos-tradicionales-de-estimación",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "",
    "text": "En las últimas décadas, se han desarrollado múltiples ecuaciones empíricas y modelos basados en datos in situ y sensores remotos para estimar esta profundidad de manera eficiente y a gran escala. El objetivo de recopilar es identificar patrones comunes, variables predictoras relevantes y estrategias metodológicas empleadas.\n\n\nTabla 1: Algoritmos publicados para la predicción de \\(\\mathrm{SDD}\\) utilizando la plataforma Landstat [1].\n\n\n\n\n\n\n\n\n\nNombre\nVariable\nFormula\nMuestras\nR²\n\n\n\n\nDekker and Peters\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{ln(Red)}\\)\n15\n0.86\n\n\nDominguez Gomez et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{(Green)^x}\\)\n16\n0.90\n\n\nGiardino et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{Blue/Green}\\)\n4\n0.85\n\n\nKloiber et al.\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{Blue/Red + Blue}\\)\n374\n0.93\n\n\nLathrop and Lillesand\n\\(\\mathrm{ln(SDD)}\\)\n\\(\\mathrm{Green}\\)\n9\n0.98\n\n\nMancino et al.\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{Red/Green + Blue/Green + Blue}\\)\n60\n0.82\n\n\n\n\nEn la mayoría de los casos la relación entre \\(SDD\\) y la intensidad de la luz es no lineal, por lo que se utiliza \\(ln(SDD)\\) para realizar la regresión.\nLa correlación con la banda roja puede explicarse causalmente por la correlación positiva directa entre la reflectancia en el rojo y la carga bruta de partículas que induce la dispersión de partículas. De manera que mientras que la claridad del agua \\((SDD)\\) desciende, la intensidad en el rojo aumenta [2].\nComo referencia se presentan las propiedades de las distintas bandas de la plataforma espacial Sentinel-2:\n\n\nTabla 2: Propiedades de las bandas S2-MSI, para las plataformas S2A y S2B.\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-2A\n\nSentinel-2B\n\n\n\n\n\nBanda\nResolución espacial (m)\nLongitud de onda (nm)\nAncho de banda (nm)\nLongitud de onda (nm)\nAncho de banda (nm)\n\n\n\\(\\mathrm{B01}\\) (aerosol)\n60\n442.7\n20\n442.3\n20\n\n\n\\(\\mathrm{B02}\\) (blue)\n10\n492.7\n65\n492.3\n65\n\n\n\\(\\mathrm{B03}\\) (green)\n10\n559.8\n35\n558.9\n35\n\n\n\\(\\mathrm{B04}\\) (red)\n10\n664.6\n38\n664.9\n31\n\n\n\\(\\mathrm{B05}\\) (red edge)\n20\n794.1\n14\n703.8\n15\n\n\n\\(\\mathrm{B06}\\)\n20\n748.5\n14\n739.1\n13\n\n\n\\(\\mathrm{B07}\\)\n20\n782.8\n19\n779.7\n19\n\n\n\\(\\mathrm{B08}\\) (NIR)\n10\n832.8\n105\n832.9\n104\n\n\n\\(\\mathrm{B8A}\\)\n20\n864.7\n21\n864.0\n21\n\n\n\\(\\mathrm{B09}\\)\n60\n945.1\n19\n943.2\n20\n\n\n\\(\\mathrm{B10}\\)\n60\n1373.5\n29\n1376.9\n29\n\n\n\\(\\mathrm{B11}\\) (SWIR 1)\n20\n1613.7\n90\n1616.4\n94\n\n\n\\(\\mathrm{B12}\\) (SWIR 2)\n20\n2292.4\n174\n2185.7\n184"
  },
  {
    "objectID": "desarrollo.html#regresión-lineal",
    "href": "desarrollo.html#regresión-lineal",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "",
    "text": "En estadística, la regresión lineal es un modelo matemático usado para aproximar la relación de dependencia entre una variable dependiente \\(Y\\) con \\(m\\) variables independientes \\(X_i\\). Este modelo puede ser expresado como:\n\\[ Y = \\beta_0 +  \\beta_1 X_1 + \\cdots + \\beta_m X_m\\]\ndonde:\n\n\\(Y\\) es la variable dependiente o variable de respuesta.\n\\(X_1,X_2,...X_m\\) son las variables explicativas, independientes o regresoras.\n\\(\\beta_0,\\beta_1,\\beta_2,...\\beta_m\\) son los parámetros del modelo, miden la influencia que las variables explicativas tienen sobre el regrediendo.\n\n\n\nLa seleccion de variables y características es el foco de mucha investigación. El objetivo de la selección de variables es mejorar el rendimiento de predicción de los predictores y proporcionar una mejor comprensión del proceso subyacente que generó los datos.\nLa identificación de las variables más relevantes para incluir o excluir en un modelo predictivo es un paso fundamental en cualquier investigación rigurosa, especialmente cuando se busca construir modelos con alto poder explicativo y capacidad de generalización.\nComprender qué variables contribuyen en mayor medida a la calidad de las predicciones permite no solo mejorar el desempeño del modelo, sino también interpretar con mayor claridad los fenómenos que se están estudiando.\nNo existe una forma segura de definir la importancia de una variable puesto que la utilidad de la misma depende del modelo implementado y de las demás variables con las que interactua: una variable que es completamente inutil por si misma puede resultar en una mejora del rendimiento significativa cuando es considerada junto con otras variables [3].\nA pesar de las dificultades existen estrategias útiles para determinar un subconjunto de variables útiles para la predicción. Tal es el caso de los métodos forward selection y backwards elimination. En forward selection, las variables son progresivamente incorporadas, evaluando el modelo al paso de cada una, mientras que en backwards elimination uno empieza con el conjunto entero de variables y progresivamente elimina las menos prometedoras.\nUna forma de implementación de forward selection es por empezar por un modelo exento de variables independientes, pero cuyo término independiente sea la media de las variables de respuesta. A partir de este valor se calcula el residuo como la diferencia entre el valor verdadero \\(Y\\) y el valor predecido \\(Y_{pred}\\).\n\\[r = Y - Y_{pred}\\]\nSeguidamente, se computa la correlación entre cada variable independiente \\(X_i\\) con el residuo y se incorpora al modelo la variable con la mayor correlación absoluta. El proceso se repite esta vez con las predicciones del nuevo modelo y se continua hasta alcanzar una desempeño deseado o un número determinado de variables. Este método fue utilizado satisfactoriamente en investigaciones previas [4].\n\n\n\nFig. 1: Matriz de correlación entre las distintas bandas.\n\n\nUna herramienta útil para ayudar en la selección de variables es una matriz de correlación. Dos variables prefectamente correlacionadas resultan redundantes en el sentido de que añadir ambas no aporta información adicional. Sin embargo, una correlación muy alta entre variables (o anti-correlación) no significa ausencia de complementariedad.\n\n\n\nIncluir nuevas variables a partir de las originales, como productos, cocientes o transformaciones no lineales (por ejemplo, logaritmos, o raíces cuadradas) permite capturar relaciones más complejas entre las variables independientes y la variable objetivo. Por ejemplo, si se dispone de datos espectrales, pueden construirse razones entre bandas (band ratios) o índices espectrales que resalten ciertas propiedades físicas del fenómeno estudiado. Estas nuevas características permiten al modelo lineal aproximar mejor la relación entre los datos y la salida, especialmente cuando la relación real no es perfectamente lineal.\n\n\n\nLas métricas de desempeño permiten cuantificar qué tan bien un modelo se ajusta a los datos y predice resultados. Algunas métricas evalúan la calidad del ajuste, otras penalizan la complejidad y algunas miden directamente el error de predicción.\nEl Coeficiente de Determinación Ajustado \\(R^2-ADJ\\) es una variante de R² que corrige la tendencia de este último a aumentar al añadir más variables al modelo, incluso si no aportan mejora real. Mientras que el R² tradicional mide la proporción de variabilidad de la variable dependiente explicada por el modelo, el ADJ-R²penaliza la inclusión de predictores irrelevantes, ajustando el valor según el número de observaciones y el número de variables independientes. Esto lo hace más útil para comparar modelos con distinto número de predictores, evitando la falsa impresión de mejora solo por complejidad adicional.\nEl Criterio de Información de Akaike (AIC) es una métrica utilizada para comparar modelos estadísticos en términos de ajuste y complejidad. Se basa en la teoría de la entropía y busca identificar el modelo que mejor explica los datos con el menor número posible de parámetros. Un valor de AIC más bajo indica un mejor equilibrio entre ajuste y simplicidad. La fórmula general es:\n\\[AIC = 2k - 2 ln(L)\\]\ndonde \\(k\\) es el número de parámetros estimados en el modelo y \\(L\\) es el valor máximo de la función de verosimilitud. A diferencia del ADJ-R² el AIC no tiene un valor máximo fijo y es más útil cuando se comparan múltiples modelos candidatos sobre el mismo conjunto de datos.\nEl Error Cuadrático Medio (MSE) mide el promedio de los cuadrados de las diferencias entre los valores observados y los predichos por el modelo. Es una métrica sensible a errores grandes, ya que los eleva al cuadrado, y por eso penaliza más las predicciones alejadas de la realidad. Su fórmula es:\n\\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\ndonde \\(y_i\\) y son \\(\\hat{y}_i\\) los valores reales y los valores predichos. Un MSE más bajo indica un mejor desempeño del modelo, pero su escala depende de la magnitud de la variable objetivo, por lo que no siempre es directamente interpretable sin contexto."
  },
  {
    "objectID": "desarrollo.html#índices-espectrales",
    "href": "desarrollo.html#índices-espectrales",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "",
    "text": "Los índices espectrales son combinaciones matemáticas de reflectancias en distintas bandas del espectro electromagnético obtenidas por sensores remotos, como satélites o drones. Estos índices realzan ciertas características o propiedades de la superficie observada, facilitando la identificación y monitoreo de elementos como vegetación, agua, suelo o áreas urbanas. Al reducir la información a una sola variable combinada, permiten detectar cambios y evaluar condiciones ambientales con mayor precisión que el análisis de bandas individuales.\n\n\nEl Índice de Diferencia Normalizada del Agua (NDWI) es un índice espectral diseñado para resaltar la presencia y extensión de cuerpos de agua en imágenes satelitales. Se calcula utilizando reflectancias en bandas específicas del verde y el infrarrojo cercano, y su fórmula clásica es:\n\\[NDWI = \\frac{R_{\\text{verde}} - R_{\\text{NIR}}}{R_{\\text{verde}} + R_{\\text{NIR}}}\\]\ndonde $R_{verde} es la reflectancia en la banda verde y \\(R_{NIR}\\) la reflectancia en la banda del infrarrojo cercano. Valores altos de NDWI indican presencia probable de agua, ya que el agua absorbe fuertemente en el NIR y refleja más en el verde."
  },
  {
    "objectID": "desarrollo.html#librerías",
    "href": "desarrollo.html#librerías",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "2.1 Librerías",
    "text": "2.1 Librerías\nPara regresión lineal se usan principalmente las librerías de Python numpy, pandas y Scikit-learn.\n\nScikit-learn: está en el corazón de las operaciones de ciencias de datos en Python. Ofrece módulos para procesamiento de datos, aprendizaje supervisado y no supervisado, selección y validación de modelos, y métricas de error.\nPandas: especializada en la manipulación y el análisis de datos. Ofrece estructuras de datos y operaciones para manipular tablas numéricas y series temporales.\nNumPy: provee al usuario con arreglos multidimensionales, junto a un gran conjunto de funciones para operar en estos."
  },
  {
    "objectID": "desarrollo.html#lectura-de-datos",
    "href": "desarrollo.html#lectura-de-datos",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "2.2 Lectura de datos",
    "text": "2.2 Lectura de datos\nPara la carga, lectura y manipulación de la información la librería Pandas permite convertir un archivo .csv en un DataFrame, estructura de datos que facilita la manipulación de estos. El fragmento de código siguiente agrupa los valores de \\(SDD\\) con los valores de reflectancia a diferentes longitudes de onda para los cuales coinciden fecha, latitud y longitud y crea un nuevo DataFrame.\n\nimport pandas as pd\n\n# Rutas a los archivos CSV\narchivo_reflectancias = \"datos\\\\base_de_datos_gis.csv\"  # contiene: fecha,punto,pixel,banda,reflect,longitud,latitud\narchivo_parametros = \"datos\\\\base_de_datos_lab.csv\"     # contiene: fecha,longitud,latitud,param,valor\n\n# Leer los archivos\ndf_reflect = pd.read_csv(archivo_reflectancias)\ndf_param = pd.read_csv(archivo_parametros)\n\n# Filtrar los parámetros \"secchi\"\ndf_secchi = df_param[df_param[\"param\"].str.lower() == \"secchi\"]\n\n# Merge por fecha y coordenadas\nmerged = pd.merge(\n    df_secchi,\n    df_reflect,\n    on=[\"fecha\", \"latitud\", \"longitud\"],\n    how=\"inner\"\n)\n\n# Pivotear la tabla para poner bandas como columnas\ntabla_final = merged.pivot_table(\n    index=[\"param\", \"fecha\", \"longitud\", \"latitud\", \"valor\"], \n    columns=\"banda\",\n    values=\"reflect\"\n).reset_index()\n\n# Reordenar columnas: param | B01 | B02 | ... | B8A\nbandas = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12', 'B8A']\ncolumnas_finales = ['valor'] + bandas\n\n# Crear Tabla final\ndf = tabla_final[columnas_finales]\n\n# Guardar excel para pruebas finales\nsalida_excel = \".//datos//tabla_final.xlsx\"\ndf.to_excel(salida_excel, index=False)\n\nEl DataFrame obtenido se asemeja a la siguiente tabla:\n\n\nTabla 3: DataFrame de los datos recolectados.\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{SDD}\\)\n\\(\\mathrm{B01}\\)\n\\(\\mathrm{B02}\\)\n\\(\\cdots\\)\n\\(\\mathrm{B12}\\)\n\\(\\mathrm{B8A}\\)\n\n\n\n\n10\n0,1728\n0,1754\n\\(\\cdots\\)\n0,1404\n0,1869\n\n\n15\n0,1497\n0,17022\n\\(\\cdots\\)\n0,1113\n0,1567\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\cdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n125\n0,1571\n0,1563\n\\(\\cdots\\)\n0,1419\n0,1436\n\n\n135\n0,1503\n0,1591\n\\(\\cdots\\)\n0,1420\n0,1454"
  },
  {
    "objectID": "desarrollo.html#análisis-de-los-datos",
    "href": "desarrollo.html#análisis-de-los-datos",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "2.3 Análisis de los datos",
    "text": "2.3 Análisis de los datos\nLa relación no lineal entre la penetración de la luz y la profundidad de disco de Secchi fue ya descrita por diversos autores que hallan una mejor descripción de esta como una de tipo logaritmica [5]. Se verifica este comportamiento con las bandas B04, B05 y B06:"
  },
  {
    "objectID": "desarrollo.html#desarrollo-del-modelo",
    "href": "desarrollo.html#desarrollo-del-modelo",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "2.4 Desarrollo del modelo",
    "text": "2.4 Desarrollo del modelo\nPara el desarrollo del modelo se implementó la clase RegresionLineal, que contiene el código necesario para la construcción y selección de variables así como para la validación cruzada del modelo. La misma requiere unicamente de los arrays correspondientes a la variable objetivo y a las características con las que estimarla. El script se encuentra disponible acá.\nSe realiza la búsqueda de ecuaciones utilizando los valores de reflectancia corregidos por SEN2COR y por ACOLITE de manera separada.\n\n2.4.1 Corrección por SEN2COR\nLa implementación del método sepwise forward selection permite evaluar la adición de variables al modelo predictivo en una instancia inicial donde no se general características adicionales. Los resultados de cada paso se encuentran resumidos en la siguiente tabla:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.0263\n-0.0263\n44.7372\n119.03\n\n\n1\nB05\n0.2761\n0.2221\n37.6319\n113.56\n\n\n2\nB02\n0.7158\n0.6701\n23.2318\n99.69\n\n\n3\nB08\n0.7614\n0.6989\n21.3544\n99.41\n\n\n4\nB03\n0.8165\n0.7459\n18.7613\n97.38\n\n\n5\nB04\n0.8296\n0.7383\n17.9258\n97.47\n\n\n\n\n\nLa combinación de B05, B02 y B08 resulta en el mejor ajuste para valores mínimos de RMSE y AIC.\nHaciendo uso de la relación logaritmica entre la profundidad de disco y la penetración de la luz descrita y probada previamente se obtiene un modelo con una notable mejoría:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nB05\n0.2949\n0.2421\n36.9801\n112.79\n\n\n2\nB02\n0.8333\n0.8063\n17.7159\n91.89\n\n\n3\nB08\n0.8379\n0.7950\n17.4478\n93.32\n\n\n4\nB11\n0.8519\n0.7944\n16.5047\n93.56\n\n\n5\nB07\n0.8549\n0.7768\n16.5084\n95.74\n\n\n\n\n\nLa mejoría se extiende hasta la adición de la variable B07, después de la cual hay una ligera disminución de R²-ADJ, pero no lo suficientemente significativa como para tratarse de un caso de sobreajuste considerable.\nLa aplicación del logaritmo puede extenderse a las variables mismas, resultando en un modelo predictivo mucho más certero con una menor cantidad de variables:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nlog(B05)\n0.4339\n0.3916\n32.8131\n108.64\n\n\n2\nlog(B02)\n0.8834\n0.8644\n15.0118\n87.21\n\n\n3\nB06\n0.8934\n0.8651\n14.3537\n87.65\n\n\n4\nB08\n0.9014\n0.8631\n13.6956\n88.17\n\n\n5\nB12\n0.9093\n0.8606\n13.0846\n88.57\n\n\n\n\n\nLa combinación de log(B05) y B02 resulta prometedora: el valor de AIC es mínimo, el RMSE es bajo, y el R² alto. Los coeficientes y ordenada de la ecuación resultante son:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nlog(B05)\n0.4339\n0.3916\n32.8131\n108.64\n\n\n2\nlog(B02)\n0.8834\n0.8644\n15.0118\n87.21\n\n\n\n\n\nLos coeficientes y ordenada resultantes de considerar a las primeras dos variables son:\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\nlog(B05)\n-12.260673\n\n\nlog(B02)\n13.030511\n\n\nOrdenada\n5.275337\n\n\n\n\n\nEl uso de cocientes entre las variables otorga una cantidad mucho más extensiva de predictores, observamos que de considerar a estos junto con las variables linealizadas, estos predominan como mejores contribuyentes a la mejora de la predicción:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.1888\n-0.1888\n48.1871\n121.19\n\n\n1\nB05/B02\n0.8822\n0.8734\n15.0707\n84.77\n\n\n2\nlog(B01)\n0.8966\n0.8798\n14.0689\n84.36\n\n\n3\nB08/B06\n0.9091\n0.8850\n13.1732\n84.66\n\n\n4\nB8A/B11\n0.9100\n0.8750\n13.0816\n86.29\n\n\n5\nB07/B11\n0.9087\n0.8596\n13.1518\n88.25\n\n\n\n\n\nLos coeficientes y ordenada resultantes de considerar a las primeras dos variables son:\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\nB05/B02\n-4.268558\n\n\nlog(B01)\n0.643369\n\n\nOrdenada\n9.430494\n\n\n\n\n\n\n\n2.4.2 Corrección por ACOLITE\nSe repite el proceso que con los datos corregidos por el algoritmo SEN2COR. Se menciona que en la lectura y tratamiento de datos se descartaron los puntos para los cuales los valores de reflectancia eran negativos luego de la corrección.\nPartimos de haber hallado que la aplicación del logaritmo a la profundidad de disco resulta en un mejor ajuste. En relación a los valores corregidos por ACOLITE esto devuelve los siguientes resultados:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.2303\n-0.2303\n47.8282\n59.46\n\n\n1\nB05\n0.1189\n-0.0378\n38.8925\n56.79\n\n\n2\nB02\n0.6160\n0.4521\n27.3299\n51.54\n\n\n3\nB08\n0.7201\n0.4925\n23.1573\n51.70\n\n\n4\nB03\n0.8162\n0.5245\n16.6068\n48.02\n\n\n5\nB06\n0.8334\n0.2594\n16.2201\n49.84\n\n\n\n\n\nObservamos que para las mismas combinaciones los valores de R² y R²-ADJ aumentaron significativamente respecto a aquellos con la corrección por SEN2COR.\nPara una relación doblemente logarítimica, obtenemos lo siguiente:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.2303\n-0.2303\n47.8282\n59.46\n\n\n1\nlog(B05)\n0.3287\n0.2083\n28.9200\n52.62\n\n\n2\nlog(B02)\n0.6888\n0.5550\n21.0484\n49.21\n\n\n3\nB01\n0.7729\n0.5845\n18.4856\n48.91\n\n\n4\nlog(B08)\n0.8185\n0.5269\n15.6606\n48.45\n\n\n5\nB06\n0.8315\n0.2347\n15.0506\n50.27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\n0\nlog(B05)\n-5.058134\n\n\n1\nlog(B02)\n6.861923\n\n\n2\nB01\n-18.872053\n\n\n3\nOrdenada\n7.577205\n\n\n\n\n\n\n\nDonde esta vez se ve una desmejora respecto al modelo corregido por SEN2COR para un modelo de dos variables logarítimicas.\nLa incorporación de cocientes al abanico de variables devuelve las siguientes métricas:\n\n\n\n\n\nPaso\nVariable\nR²\nADJ-R²\nRMSE\nAIC\n\n\n\n\n0\nNinguna\n-0.2303\n-0.2303\n47.8282\n59.46\n\n\n1\nB04/B02\n0.7325\n0.6853\n20.3765\n47.21\n\n\n2\nB04/B01\n0.7795\n0.6848\n19.1909\n48.01\n\n\n3\nB07/B06\n0.8216\n0.6737\n15.8882\n47.67\n\n\n4\nB08\n0.8167\n0.5294\n16.0595\n49.33\n\n\n5\nB11/B05\n0.8592\n0.3978\n13.5437\n48.60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCoeficiente\n\n\n\n\n0\nB04/B02\n-1.582140\n\n\n1\nOrdenada\n6.402011\n\n\n\n\n\n\n\nNuevamente, se ve una desmejora respecto al modelo corregido por SEN2COR. El número de variables se reduce a una."
  },
  {
    "objectID": "desarrollo.html#ecuaciones-halladas",
    "href": "desarrollo.html#ecuaciones-halladas",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "2.5 Ecuaciones halladas",
    "text": "2.5 Ecuaciones halladas\nEl par de mejores ecuaciones halladas para cada algoritmo de corrección utilizando la transformación logarítmica fueron las siguientes: \\[ log(SDD)_{SEN2COR} = -12.26 \\cdot log(B05) + 13.03 \\cdot log(B02) + 5.27 \\]\n\\[ log(SDD)_{ACOLITE} = -5.06 \\cdot log(B05) + 6.86 \\cdot log(B02) -18.87 \\cdot B01 \\]\nSus ajustes se visualizan de la siguiente manera:\n\n\n\n\n\n\n\n\n\nEl par de mejores ecuaciones halladas para cada algoritmo utilizando cocientes entre las variables es:\n\\[ log(SDD)_{SEN2COR} = -4.27\\frac{B05}{B02} + 0.64\\cdot log(B01)+ 9.43 \\]\n\\[ log(SDD)_{ACOLITE} = -1.58\\frac{B04}{B02} + 6.40 \\]\nSus ajustes se visualizan de la siguiente manera:\n\n\n\n\n\n\n\n\n\nObservamos que la utilización del algoritmo de ACOLITE para la corrección atmosférica ocasiona una desmejora en la estimación."
  },
  {
    "objectID": "desarrollo.html#mapas-de-la-estimación",
    "href": "desarrollo.html#mapas-de-la-estimación",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "2.6 Mapas de la estimación",
    "text": "2.6 Mapas de la estimación\nLos mapas generados para cada fecha corresponden al recorte del área de interés del producto satelital corregido por ACOLITE.\n\n06/09/2516/12/2430/05/2420/05/2412/12/2311/05/23"
  },
  {
    "objectID": "desarrollo.html#random-forest",
    "href": "desarrollo.html#random-forest",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "3.1 Random Forest",
    "text": "3.1 Random Forest\nRandom Forest es un algoritmo de aprendizaje automático basado en un conjunto de árboles de decisión entrenados con muestras y variables aleatorias. La combinación de múltiples árboles reduce la varianza y mejora la capacidad de generalización del modelo. Es robusto frente al sobreajuste y adecuado para capturar relaciones no lineales entre las variables.\n\n3.1.1 Hiperparámetros\nLos hiperparámetros son valores que se definen antes del entrenamiento de un modelo y controlan cómo aprende. Ajustarlos correctamente permite mejorar el rendimiento y la capacidad de generalización del modelo.\n\nn_estimators es el número total de árboles que se crearán en el bosque aleatorio. Cada árbol se entrena con una muestra distinta de los datos (si bootstrap=True) y sus predicciones se promedian. Aumentar este número hace que el modelo sea más estable y consistente, reduciendo la varianza y mejorando la precisión. Sin embargo, también incrementa el tiempo de entrenamiento y el uso de memoria, por lo que hay que buscar un equilibrio.\nmax_depth determina la cantidad máxima de niveles que puede tener cada árbol, es decir, cuántas divisiones sucesivas puede hacer desde la raíz hasta una hoja. Cuanto más profundo sea un árbol, más patrones complejos puede aprender, pero también corre el riesgo de memorizar el ruido de los datos (sobreajuste). Por el contrario, limitar la profundidad hace que los árboles sean más simples y generalicen mejor, aunque podrían no capturar todas las relaciones presentes en los datos.\nmin_samples_split indica el número mínimo de muestras necesarias para dividir un nodo en dos ramas. Este parámetro evita que el árbol realice divisiones basadas en muy pocos datos, que podrían ser poco representativos. Valores pequeños permiten divisiones frecuentes, generando árboles más complejos y propensos al sobreajuste, mientras que valores mayores producen divisiones más conservadoras y árboles más suaves que generalizan mejor.\nmin_samples_leaf establece el mínimo de muestras que debe contener una hoja, es decir, un nodo terminal del árbol. Si se permite una sola muestra por hoja, el árbol puede memorizar los datos y generar predicciones muy específicas pero poco generalizables. Aumentar este valor hace que cada hoja represente un promedio de varias observaciones, produciendo predicciones más estables y menos sensibles al ruido.\nmax_features determina cuántas variables se consideran al buscar la mejor división en cada nodo. Limitar la cantidad de features introduce aleatoriedad en cada árbol, lo que reduce la correlación entre ellos y mejora la generalización del bosque. Por ejemplo, “sqrt” utiliza la raíz cuadrada del total de variables y “log2” utiliza el logaritmo base 2 del total de variables. La diferencia es sutil pero importante: “log2” suele considerar menos variables en cada división, aumentando la diversidad entre árboles, mientras que “sqrt” toma un número ligeramente mayor, equilibrando diversidad y cantidad de información.\nbootstrap indica si cada árbol se entrena con una muestra aleatoria con reemplazo del conjunto total de datos. Si es True, cada árbol ve datos diferentes, lo que genera más diversidad entre árboles y mejora la capacidad de generalización. Si es False, todos los árboles utilizan el mismo conjunto, lo que puede hacerlos más parecidos entre sí y aumentar la posibilidad de sobreajuste. Además, usar bootstrap permite calcular errores de tipo “out-of-bag” para evaluar el modelo sin necesidad de un conjunto de validación separado.\n\n\n\n3.1.2 Resultados\n\nEl modelo Random Forest mostró un buen nivel de ajuste y capacidad predictiva, capturando adecuadamente las relaciones entre variables. Sin embargo, presentó un RMSE más alto en comparación con los modelos lineales, lo que indica mayor dispersión en las predicciones y una menor precisión promedio pese a su buen desempeño general. La importancia de cada variable fue la siguiente:\n\nLas bandas B04 y B04 predominan en el ranking, esto puede deberse a que las bandas Red y Red Edge (B05–B06-B07) se ubican justo en la transición entre el rojo y el infrarrojo cercano, donde la reflectancia del agua cambia mucho con la concentración de sedimentos. Esto las hace excelentes indicadores de la turbidez y material particulado suspendido.\nCon el objetivo de mejorar el desempeño de los modelos, se incorporaron cocientes entre variables como nuevas características predictoras. Estas relaciones permiten resaltar contrastes espectrales y capturar patrones relativos entre bandas, lo que puede aumentar la sensibilidad del modelo frente a variaciones en la transparencia del agua y mejorar la estimación.\n\nAl introducir los cocientes entre variables como nuevas características, se observa una mejora notable en el desempeño de los modelos. En particular, el RMSE se redujo hasta menos de la mitad, indicando predicciones más precisas y consistentes"
  },
  {
    "objectID": "desarrollo.html#xgboost",
    "href": "desarrollo.html#xgboost",
    "title": "Desarrollo integral de las ecuaciones y modelos de regresión",
    "section": "3.2 XGBoost",
    "text": "3.2 XGBoost\nXGBoost (Extreme Gradient Boosting) es un método de boosting que construye árboles de manera secuencial, donde cada uno corrige los errores del anterior. Incorpora técnicas de regularización y optimización que aumentan la precisión y evitan el sobreajuste, ofreciendo un excelente desempeño en tareas de regresión y clasificación.\n\n3.2.1 Hiperparámetros\nAdemás de los hiperparámetros n_estimators y max_depth ya vistos para Random Forest, XGBoost cuenta con los siguientes hiperparámetros:\n\nlearning_rate controla cuánto contribuye cada árbol nuevo a la predicción total. Valores bajos (por ejemplo, 0.05) hacen que el modelo aprenda más despacio y con mayor precisión, requiriendo más árboles (n_estimators) para converger. Valores altos aceleran el aprendizaje, pero pueden provocar sobreajuste si los árboles son profundos o abundantes.\nsubsample indica la fracción de muestras de entrenamiento que se usan para construir cada árbol. Un valor menor que 1 introduce aleatoriedad, lo que reduce la correlación entre árboles y mejora la generalización, pero demasiado bajo puede hacer que el modelo no aprenda bien. Un valor de 1 significa que todos los datos se usan en cada árbol, maximizando ajuste pero con menor diversidad.\ncolsample_bytree define la fracción de variables (features) consideradas al entrenar cada árbol. Limitar las columnas aumenta la aleatoriedad entre árboles, reduciendo la correlación y el sobreajuste. Por ejemplo, 0.8 significa que cada árbol ve solo el 80% de las variables disponibles.\ngamma es el mínimo incremento en la función de pérdida requerido para realizar una división adicional en un nodo. Valores mayores hacen que el árbol sea más conservador, evitando divisiones que aporten poca mejora en la predicción, lo que ayuda a reducir sobreajuste.\nreg_alpha es la penalización de tipo L1 sobre los pesos de los árboles. Ayuda a reducir la complejidad del modelo y puede forzar algunos pesos a cero, promoviendo sparsity y simplificación. Esto es útil si hay muchas variables poco relevantes.\nreg_lambda es la penalización de tipo L2 sobre los pesos de los árboles. Contrarresta valores extremos y ayuda a estabilizar el modelo, evitando que algunos árboles dominen la predicción y reduciendo el riesgo de sobreajuste.\n\n\n\n3.2.2 Resultados\n\nEl modelo XGBoost demuestra desde un inicio un poder predictivo mayor al de Random Forest y a su vez una dispersión mucho menor en las predicciones.\n\nNuevamente se repite la predominancia de las bandas B04 y B05 en el ranking.\n\nSe observa que la incorporación de cocientes incrementa considerablemente el poder predictivo, a la vez que el RMSE disminuye a valor comparables con el mejor de los valores lineales previamente obtenidos."
  },
  {
    "objectID": "estimacion.html",
    "href": "estimacion.html",
    "title": "",
    "section": "",
    "text": "Proximamente"
  }
]